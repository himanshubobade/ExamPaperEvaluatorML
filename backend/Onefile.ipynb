{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is running\n"
     ]
    }
   ],
   "source": [
    "print(\"Kernel is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.The study of natural language processing has been around for more than 50 years and grew out of the field of linguistic with the rise of computers.In this post, you will discover what natural language processing is and why it is so important.\n",
      "Linguistic is the scientific study of language, including its grammar, semantic, and phonetics.\n",
      "Classical linguistic involved devising and evaluation rules of language. Great progress was made on formal methods for santa and semantic, but for the most part, the interesting problems in natural language understanding resist clean mathematical formalisms.Broadly, a linguistic is anyone who studies language, but perhaps more colloquially, a self-defining linguistic may be more focused on being out in the field.Mathematics is the tool of science. Mathematicians working on natural language may refer to their study as mathematical linguistic, rousing exclusively on the use of discrete mathematical formalisms and theory for natural language (e.g. formal languages and automatic theory).Computational linguistic is the modern study of linguistic using the tools of computer science. Yesterday’s linguistic may be today’s computational linguistic as the use of computational tools and thinking has overtaken most fields of study.Computational linguistic is the study of computer systems for understanding and generation natural language. … One natural function for computational linguistic would be the testing of grammar proposed by theoretical linguistic.Large data and fast computers mean that new and different things can be discovered from large datasets of text by writing and running software.In the 1990s, statistical methods and statistical machine learning began to and eventually replaced the classical top-down rule-based approaches to language, primarily because of their better results, speed, and robustness. The statistical approach to studying natural language now dominated the field; it may define the field.Data-Drive methods for natural language processing have now become so popular that they must be considered mainstream approaches to computational linguistic. A strong contributing factor to this development is undoubtedly the increase amount of available electronically stored data to which these methods can be applied; another factor might be a certain disenchantment with approaches relying exclusively on hand-drafted rules, due to their observed brittleness.The statistical approach to natural language is not limited to statistics per-se, but also to advanced inference methods like those used in applied machine learning.understanding natural language require large amounts of knowledge about morphology, santa, semantic and pragmatics as well as general knowledge about the world. Acquiring and encoding all of this knowledge is one of the fundamental impediment to developing effective and robust language systems. Like the statistical methods … machine learning methods off the promise of automatic the acquisition of this knowledge from annotated or unannotated language corpora.Computational linguistic also became known by the name of natural language process, or NLP, to reflect the more engineer-based or empirical approach of the statistical methods.The statistical dominance of the field also often leads to NLP being described as Statistical Natural Language Processing, perhaps to distance it from the classical computational linguistic methods.I view computational linguistic as having both a scientific and an engineering side. The engineering side of computational linguistic, often called natural language processing (NLP), is largely concerned with building computational tools that do useful things with language, e.g., machine translation, summarization, question-answering, etc. Like any engineering discipline, natural language processing draws on a variety of different scientific discipline.Linguistic is a large topic of study, and, although the statistical approach to NLP has shown great success in some areas, there is still room and great benefit from the classical top-down methods.Roughly speaking, statistical NLP associates improbabilities with the alternatives encountered in the course of analyzing an utterance or a text and accepts the most probable outcome as the correct one.Not surprisingly, words that name phenomena that are closely related in the world, or our perception of it, frequently occur close to one another so that crisp facts about the world are reflected in somewhat funnier facts about texts. There is much room for debate in this view.Is machine learning practitioner interested in working with text data, we are concerned with the tools and methods from the field of Natural Language Processing.He have seen the path from linguistic to NLP in the previous section. Now, let’s take a look at how modern researches and practitioner define what NLP is all about.In perhaps one of the more widely textbook written by top researches in the field, they refer to the subject as linguistic science, permitting discussion of both classical linguistic and modern statistical methods.The aim of a linguistic science is to be able to characterize and explain the multitude of linguistic observations curling around us, in conversations, writing, and other media. Part of that has to do with the cognitive size of how humans acquire, produce and understand language, part of it has to do with understanding the relationship between linguistic utterances and the world, and part of it has to do with understand the linguistic structures by which language communicates.They go on to focus on inference through the use of statistical methods in natural language processing. Statistical NLP aims to do statistical inference for the field of natural language. Statistical inference in general consists of taking some data (generate in accordance with some unknown probability distribution) and then making some inference about this distribution.In their text on applied natural language processing, the authors and contributory to the popular NLTK Python library for NLP describe the field broadly as using computers to work with natural language data.He will take Natural Language Processing — or NLP for short –in a wide sense to cover any kind of computer manipulation of natural language. It one extreme, it could be as simple as counting word frequencies to compare different writing style. It the other extreme, NLP involves “understanding” complete human utterances, at least to the extent of being able to give useful responses to them. Statistical NLP has turned another corner and is now strongly focused on the use of deep learning neutral network to both perform inference on specific tasks and for developing robust end-to-end systems.In one of the first textbook dedicated to this emerging topic, Road Goldberg succinct defines NLP as automatic methods that take natural language as input or produce natural language as output.\n",
      "[TextBlob(\"Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.The study of natural language processing has been around for more than 50 years and grew out of the field of linguistic with the rise of computers.In this post, you will discover what natural language processing is and why it is so important.\"), TextBlob(\"Linguistic is the scientific study of language, including its grammar, semantic, and phonetics.\"), TextBlob(\"Classical linguistic involved devising and evaluation rules of language. Great progress was made on formal methods for santa and semantic, but for the most part, the interesting problems in natural language understanding resist clean mathematical formalisms.Broadly, a linguistic is anyone who studies language, but perhaps more colloquially, a self-defining linguistic may be more focused on being out in the field.Mathematics is the tool of science. Mathematicians working on natural language may refer to their study as mathematical linguistic, rousing exclusively on the use of discrete mathematical formalisms and theory for natural language (e.g. formal languages and automatic theory).Computational linguistic is the modern study of linguistic using the tools of computer science. Yesterday’s linguistic may be today’s computational linguistic as the use of computational tools and thinking has overtaken most fields of study.Computational linguistic is the study of computer systems for understanding and generation natural language. … One natural function for computational linguistic would be the testing of grammar proposed by theoretical linguistic.Large data and fast computers mean that new and different things can be discovered from large datasets of text by writing and running software.In the 1990s, statistical methods and statistical machine learning began to and eventually replaced the classical top-down rule-based approaches to language, primarily because of their better results, speed, and robustness. The statistical approach to studying natural language now dominated the field; it may define the field.Data-Drive methods for natural language processing have now become so popular that they must be considered mainstream approaches to computational linguistic. A strong contributing factor to this development is undoubtedly the increase amount of available electronically stored data to which these methods can be applied; another factor might be a certain disenchantment with approaches relying exclusively on hand-drafted rules, due to their observed brittleness.The statistical approach to natural language is not limited to statistics per-se, but also to advanced inference methods like those used in applied machine learning.understanding natural language require large amounts of knowledge about morphology, santa, semantic and pragmatics as well as general knowledge about the world. Acquiring and encoding all of this knowledge is one of the fundamental impediment to developing effective and robust language systems. Like the statistical methods … machine learning methods off the promise of automatic the acquisition of this knowledge from annotated or unannotated language corpora.Computational linguistic also became known by the name of natural language process, or NLP, to reflect the more engineer-based or empirical approach of the statistical methods.The statistical dominance of the field also often leads to NLP being described as Statistical Natural Language Processing, perhaps to distance it from the classical computational linguistic methods.I view computational linguistic as having both a scientific and an engineering side. The engineering side of computational linguistic, often called natural language processing (NLP), is largely concerned with building computational tools that do useful things with language, e.g., machine translation, summarization, question-answering, etc. Like any engineering discipline, natural language processing draws on a variety of different scientific discipline.Linguistic is a large topic of study, and, although the statistical approach to NLP has shown great success in some areas, there is still room and great benefit from the classical top-down methods.Roughly speaking, statistical NLP associates improbabilities with the alternatives encountered in the course of analyzing an utterance or a text and accepts the most probable outcome as the correct one.Not surprisingly, words that name phenomena that are closely related in the world, or our perception of it, frequently occur close to one another so that crisp facts about the world are reflected in somewhat funnier facts about texts. There is much room for debate in this view.Is machine learning practitioner interested in working with text data, we are concerned with the tools and methods from the field of Natural Language Processing.He have seen the path from linguistic to NLP in the previous section. Now, let’s take a look at how modern researches and practitioner define what NLP is all about.In perhaps one of the more widely textbook written by top researches in the field, they refer to the subject as linguistic science, permitting discussion of both classical linguistic and modern statistical methods.The aim of a linguistic science is to be able to characterize and explain the multitude of linguistic observations curling around us, in conversations, writing, and other media. Part of that has to do with the cognitive size of how humans acquire, produce and understand language, part of it has to do with understanding the relationship between linguistic utterances and the world, and part of it has to do with understand the linguistic structures by which language communicates.They go on to focus on inference through the use of statistical methods in natural language processing. Statistical NLP aims to do statistical inference for the field of natural language. Statistical inference in general consists of taking some data (generate in accordance with some unknown probability distribution) and then making some inference about this distribution.In their text on applied natural language processing, the authors and contributory to the popular NLTK Python library for NLP describe the field broadly as using computers to work with natural language data.He will take Natural Language Processing — or NLP for short –in a wide sense to cover any kind of computer manipulation of natural language. It one extreme, it could be as simple as counting word frequencies to compare different writing style. It the other extreme, NLP involves “understanding” complete human utterances, at least to the extent of being able to give useful responses to them. Statistical NLP has turned another corner and is now strongly focused on the use of deep learning neutral network to both perform inference on specific tasks and for developing robust end-to-end systems.In one of the first textbook dedicated to this emerging topic, Road Goldberg succinct defines NLP as automatic methods that take natural language as input or produce natural language as output.\")]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "def correct_sentence_spelling(sentence):\n",
    "    \n",
    "    sentence = TextBlob(sentence)\n",
    "    \n",
    "    result = sentence.correct()\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "import pandas as pd\n",
    "def file_to_list(file):\n",
    "    rtn: object = []\n",
    "    file_object: object = open(file, \"r\",encoding=\"utf-8\")\n",
    "    rtn: object = file_object.read().splitlines()\n",
    "    file_object.close()\n",
    "    return list(filter(None, pd.unique(rtn).tolist())) # Remove Empty/Duplicates Values\n",
    "    pass\n",
    "\n",
    "# Example #    \n",
    "data_from_file: object = file_to_list('answer1.txt')\n",
    "    \n",
    "#sentence = \"My naem is oeter\"\n",
    "for i in range(len(data_from_file)):\n",
    "    data_from_file[i] = correct_sentence_spelling(data_from_file[i])\n",
    "    print(data_from_file[i])\n",
    "    \n",
    "print(data_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spell_answer1.txt\", \"w\") as txt_file:\n",
    "    for line in data_from_file:\n",
    "        txt_file.write(\"\".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize text\n",
    "    #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    with open(\"summarize_answer1.txt\", \"w\") as txt_file:\n",
    "        txt_file.write(\". \".join(summarize_text))\n",
    "\n",
    "generate_summary( \"spell_answer1.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix Score: 0.8312151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom gensim.models.keyedvectors import KeyedVectors\\nimport numpy as np\\nfrom DocSim import DocSim\\nimport unittest\\n\\n\\nclass DocSimTest(unittest.TestCase):\\n    @classmethod\\n    def setUpClass(cls):\\n        test_model_path = \\'./data/test_data.txt\\'\\n        cls.w2v_model = KeyedVectors.load_word2vec_format(test_model_path, binary=False)\\n        cls.stopwords = [\\'to\\', \\'an\\', \\'a\\']\\n        cls.doc_sim = DocSim(cls.w2v_model, cls.stopwords)\\n\\n    def test_vectorize_with_valid_words(self):\\n        source_doc = \\'how to delete an invoice\\'\\n        # same values dummy data will output same mean value\\n        expected = np.array([0.5, 0.5, 0.6, 0.3, 0.2, 0.1, 0.4, 0.6, 0.5, 0.5])\\n        actual = self.doc_sim.vectorize(source_doc)\\n        self.assertEqual(expected.all(), actual.all())\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocSim:\n",
    "    def __init__(self, w2v_model, stopwords=None):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords if stopwords is not None else []\n",
    "\n",
    "    def vectorize(self, doc: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identify the vector values for each word in the given document\n",
    "        :param doc:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        # PS: There are other & better ways to do it.\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector\n",
    "\n",
    "    def _cosine_sim(self, vecA, vecB):\n",
    "        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n",
    "        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n",
    "        if np.isnan(np.sum(csim)):\n",
    "            return 0\n",
    "        return csim\n",
    "\n",
    "    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):\n",
    "        \"\"\"Calculates & returns similarity scores between given source document & all\n",
    "        the target documents.\"\"\"\n",
    "        if not target_docs:\n",
    "            return []\n",
    "\n",
    "        if isinstance(target_docs, str):\n",
    "            target_docs = [target_docs]\n",
    "\n",
    "        source_vec = self.vectorize(source_doc)\n",
    "        results = []\n",
    "        for doc in target_docs:\n",
    "            target_vec = self.vectorize(doc)\n",
    "            sim_score = self._cosine_sim(source_vec, target_vec)\n",
    "            if sim_score > threshold:\n",
    "                results.append({\"score\": sim_score, \"doc\": doc})\n",
    "            # Sort results by score in desc order\n",
    "            results.sort(key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "        return results\n",
    "\n",
    "#########################\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#from DocSim import DocSim\n",
    "\n",
    "# Using the pre-trained word2vec model trained using Google news corpus of 3 billion running words.\n",
    "# The model can be downloaded here: https://bit.ly/w2vgdrive (~1.4GB)\n",
    "# Feel free to use to your own model.\n",
    "googlenews_model_path = './data/GoogleNews-vectors-negative300.bin'\n",
    "stopwords_path = \"./data/stopwords_en.txt\"\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(googlenews_model_path, binary=True)\n",
    "with open(stopwords_path, 'r') as fh:\n",
    "    stopwords = fh.read().split(\",\")\n",
    "ds = DocSim(model,stopwords=stopwords)\n",
    "\n",
    "\n",
    "\n",
    "#################################  original answer to string\n",
    "with open (\"original_solution.txt\", \"r\") as myfile:\n",
    "    original_ans=myfile.readlines()\n",
    "\n",
    "#################################\n",
    "#source_doc = \"how to delete an invoice\"\n",
    "source_doc = original_ans[0]\n",
    "#print(source_doc)\n",
    "\n",
    "################################# target answers to strings\n",
    "with open (\"summarize_answer1.txt\", \"r\") as myfile:\n",
    "    target_ans=myfile.readlines()\n",
    "\n",
    "target_docs= []\n",
    "#################################\n",
    "\n",
    "#target_docs = ['delete a invoice', 'how do i remove an invoice', \"purge an invoice\"]\n",
    "#target_docs.append(target_ans)\n",
    "target_docs = target_ans\n",
    "#print(target_docs)\n",
    "\n",
    "sim_scores = ds.calculate_similarity(source_doc, target_docs)\n",
    "\n",
    "print(\"Similarity Matrix Score:\",sim_scores[0].get('score'))\n",
    "\n",
    "########################\n",
    "'''\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from DocSim import DocSim\n",
    "import unittest\n",
    "\n",
    "\n",
    "class DocSimTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        test_model_path = './data/test_data.txt'\n",
    "        cls.w2v_model = KeyedVectors.load_word2vec_format(test_model_path, binary=False)\n",
    "        cls.stopwords = ['to', 'an', 'a']\n",
    "        cls.doc_sim = DocSim(cls.w2v_model, cls.stopwords)\n",
    "\n",
    "    def test_vectorize_with_valid_words(self):\n",
    "        source_doc = 'how to delete an invoice'\n",
    "        # same values dummy data will output same mean value\n",
    "        expected = np.array([0.5, 0.5, 0.6, 0.3, 0.2, 0.1, 0.4, 0.6, 0.5, 0.5])\n",
    "        actual = self.doc_sim.vectorize(source_doc)\n",
    "        self.assertEqual(expected.all(), actual.all())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks scored by the student out of 10:- 9\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"marks scored by the student out of 10:-\",math.ceil(10*sim_scores[0].get('score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
